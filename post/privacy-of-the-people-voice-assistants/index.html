<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>#PrivacyofthePeople: Alexa, Google, Siri, we hear for you? | Internet Freedom Foundation</title><meta name=keywords content="Privacy,PrivacyofThePeople,Data Protection,data security,voice assistant,Alexa,Siri,Google Assistant"><meta name=description content="Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user centric, rights focussed, data protection law."><meta name=author content="Anandita Mishra"><link rel=canonical href=https://archive.internetfreedom.in/post/privacy-of-the-people-voice-assistants/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/iff-logo-600x250.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://archive.internetfreedom.in/iff-icon-125.png><link rel=icon type=image/png sizes=16x16 href=https://archive.internetfreedom.in/iff-icon-125.png><link rel=icon type=image/png sizes=32x32 href=https://archive.internetfreedom.in/iff-icon-125.png><link rel=apple-touch-icon href=https://archive.internetfreedom.in/iff-icon-125.png><link rel=mask-icon href=https://archive.internetfreedom.in/iff-icon-125.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?"><meta property="og:description" content="Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user centric, rights focussed, data protection law."><meta property="og:type" content="article"><meta property="og:url" content="https://archive.internetfreedom.in/post/privacy-of-the-people-voice-assistants/"><meta property="og:image" content="https://archive.internetfreedom.in/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-01-19T07:16:45+00:00"><meta property="article:modified_time" content="2022-01-19T07:16:45+00:00"><meta property="og:site_name" content="Internet Freedom Foundation"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://archive.internetfreedom.in/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?"><meta name=twitter:description content="Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user centric, rights focussed, data protection law."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://archive.internetfreedom.in/post/"},{"@type":"ListItem","position":3,"name":"#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?","item":"https://archive.internetfreedom.in/post/privacy-of-the-people-voice-assistants/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?","name":"#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?","description":"Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user centric, rights focussed, data protection law.","keywords":["Privacy","PrivacyofThePeople","Data Protection","data security","voice assistant","Alexa","Siri","Google Assistant"],"articleBody":"            tl;dr Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user-centric, rights focussed, data protection law. In this #PrivacyOfThePeople series, we discuss concerns on consent (when and how are such devices collecting data), data retention (storage of parts of audio recordings for undefined periods), cyber security audits (malicious attacks), and data sharing and surveillance (exposure of voice recordings to Voice Assistance training personnel and law enforcement).\nBackground India is witnessing a 270% year-on-year growth in voice searches. In 2020, 60% of users in India were estimated to be interacting with voice assistants (‘VA’) on their smartphones. The Mobile Marketing Association in its ‘The Voice Playbook’ (2021) records that smart speakers are being used on a daily basis for listening to music, asking questions, checking the weather, setting a timer, setting an alarm, listening to news and so on. The report points out that the reason for such exponential growth may be convenience (voice is faster than typing) and lack of literacy in India which prompts the rural population to use VAs.\nMark Weiser who headed Xerox PARC labs explained it well when he stated that, “The most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it.” Given the ubiquity of AI assistants in our daily interactions, we analyse the privacy and security concerns on three predominant, popular deployments, Alexa, Google Assistant, Siri. The basis of this analysis is the Draft Data Protection Bill, 2021 or how an ideal data protection framework can regulate their use, while maintaining innovation and limiting harm.\nPrivacy concerns on consent and retention The first privacy concern is regarding data retention. All three VAs store a part of user data including audio recordings for an undefined period even when retention of data indefinitely is against the internationally accepted principle of storage limitation. Their data retention practices are detailed below:\n     Alexa: The data collected is associated with Amazon accounts. Amazon stores recordings by default unless users choose to delete them. They can opt-in for auto-deletion of recordings that are older than “3 or 18 months” or choose to not save records at all. Do remember though, that the recordings are still processed on the cloud servers before deletion. Also, the policy is not clear on what happens to the recordings which do not meet this cut-off. Moreover, deletion only applies to voice recordings and Amazon may “retain other records of your Alexa interactions”. Amazon has also previously admitted to holding on to data received through Alexa voice interactions despite user-deletion requests. Further, as per the policy, the actions Alexa took in response to your requests can be stored for an undefined period. That means if Alexa repeated any personally identifiable sensitive information for a user, it will be on the cloud server for as long as Amazon wants it. Google Assistant: The voice data collected through Google Assistant is associated with Google accounts. Voice recordings are not stored by default and users have an option to delete them. However, if users consent to share their recordings for the purpose of improvement of features, snippets of such recordings (after being dissociated with users’ accounts) can be saved for an undefined period to improve machine learning. Google’s policy is unclear on retention of other kinds of Google Assistant-related data, but Google’s privacy policy does allow for data retention. Siri: A users’ ‘request history’ generated as a result of using Siri is associated with a random-device-generated unique identifier and not with an Apple ID or email address. Request history includes transcripts, audio for users who have opted in to Improve Siri, Siri Data i.e., contact names, names of apps installed on the device, music etc., device specifications, approximate location etc. Voice recordings are saved on Apple’s servers only if one opts in on the ‘Improve Siri’ feature. Otherwise, there is an option of processing user-voice on the device itself. However, irrespective of this option, the transcripts of interactions are always sent to Apple to process requests. The ‘request history’ is disassociated from the unique identifier after 6 months of its creation, but can be stored for up to 2 years on Apple’s servers to help Apple improve its services. In fact, a small ‘subset’ of these requests may be stored for an undefined period. Users do have an option to ‘request’ deletion of request history (the pop-up states that the data ‘will’ be deleted), but it is not automatic.  The second privacy concern is the processing and storage of data without user knowledge and consent. VAs work based on users’ voices - it is their main feature. All the above-mentioned VAs activate upon hearing a particular activation keyword. Although some of the policies claim that the cloud servers do not store data/voice unless the activation word is detected, there is constant exchange of voice \u0026 related data between their cloud servers and the VA device. This is especially concerning in cases of false activation when data may be getting stored without actual knowledge. The process of activation followed for each VA is detailed below:\n Alexa: Alexa devices are ordinarily activated by voice on hearing the ‘wake word’ chosen by a user (e.g., Alexa, Amazon, Computer or Echo). Amazon claims that till the wake word is detected, no audio is stored or sent to the cloud. The question here is, how is the wake word detected? While that is not answered clearly in the policy, Amazon has described how it verifies a wake word. As per its policy, the Alexa-enabled device starts streaming audio to the cloud upon detection of the wake word. This is where Alexa performs a “cloud verification” to confirm its detection. If the cloud verification does not also detect the wake word, Alexa stops processing the audio and ends the audio stream to the cloud. If the wake word is detected, Alexa attempts to determine when a users’ request ends and stops the audio stream at that point. Interestingly, the policy is silent on what happens to the audio received if the wake word is not verified or incorrectly verified by Alexa. Google Assistant: As per Google’s policy, the device is on standby mode by default and only activates upon hearing words like “Hey Google” or “Ok Google.” Short snippets of audio (a few seconds) are processed to detect an activation. If no activation is detected, then those audio snippets won’t be sent or saved to Google. If an activation is detected, the Assistant comes out of standby mode to fulfil user requests. The policy is again unclear on how the activation word is detected and where the short snippets are processed. If snippets are processed on Google’s server, the policy must mention how Google deals with the data associated with those snippets. Siri: It can be activated by voice by using the words, “Hey Siri.” Siri’s policy is completely silent on how it processes the activation word. However, Siri has a peculiar feature. Siri Settings of a device clearly states if a user’s voice is processed on their device. If the same is not stated, the voice inputs are sent to and processed on Siri servers. Therefore, if the voice is being processed on the device itself, the issue of processing and storage of data without a user’s knowledge may not be that exacerbated because the device is always with the user. The policy also states that transcripts of voice recordings are always sent to Apple servers. Therefore, in cases of false detection of the activation word, even if the audio is processed on the device, processing and storage of transcripts on servers will still take place without user knowledge.  Common concerns associated with VAs The common concerns associated with all VAs independent of their policies are surveillance and security. Human involvement is essential for improvement of Natural Language Processing (allows interaction with a system in a spontaneous way) and machine learning. Employees of Apple, Google and Amazon review randomly selected parts of voice recordings to verify whether VAs processed the voice requests correctly. Training personnels also listen to the audio, transcribe it, annotate it, and convert it into material that can be used for training voice recognition algorithms. So, even when the voice snippet stored in cloud servers is not associated with an identifier on Siri, or an ID on Google Assist, if the snippet is (say) of your partner saying, “order Raj his favourite Margherita at our Lonavala house”, the training personnel hearing this information will know personally identifiable information about Raj and the speaker. Another risk of exposure is when governments, law enforcement authorities or courts demand audio recordings and associated data for the purpose of investigations.\nSecurity concerns are associated with malware attacks and hacking. Researchers have found that successful malicious attacks against VAs are becoming more and more sophisticated. The hyperlinked study summarises flaws that allow remote execution of voice commands on Alexa which can enable a malicious user to make online purchases with another user’s VA, discusses security attacks such as ‘phoneme morphing’ in which a VA is tricked into thinking a modified recording of an attacker’s voice is the device’s registered user, and ‘dolphin attack’ in which inaudible voice commands can be used to communicate with VAs, evading detection by human hearing. These attacks allow the extraction of personally identifiable information from unsecured VAs with ease. Therefore, the privacy of voice recordings depends heavily on the security of the servers on which they are stored, and the devices on which they are enabled.\nThe Data Protection Bill, 2021 and VAs After two years of its constitution the Joint Parliamentary Committee (“JPC”) on the Personal Data Protection Bill, 2019 released its report in December 2021. The report introduces Data Protection Bill, 2021 (“Bill’) incorporating the amendments and omissions suggested by the JPC.\nClause 17 provides users with the right to confirmation and access. This right will allow VA users to demand confirmation from the data fiduciaries on whether their request to delete their data has been processed. Notwithstanding the policies of the data fiduciaries, by virtue of Clause 18, a VA user can also demand erasure of personal data which is no longer necessary for the purpose for which it was processed.\nAs seen above, when VA users consent to the use of audio recordings by Amazon, Apple, and Google for improvement of services, the companies can store small anonymised snippets or subsets of the voice recordings for an undefined period. Clause 20 applies when a user wants to withdraw such consent. Upon withdrawal of consent, it gives the user the right to restrict or prevent processing of their personal data (i.e. one’s voice) by a data fiduciary (provided it does not take away the data fiduciaries rights under this Bill.)\nPursuant to Clause 50, the Data Protection Authority must specify a specific code of practice for VAs. The European Data Protection Board has adopted guidelines on virtual voice assistants (‘VVA’) which can be used as a guiding document. The guidelines bring all forms of interaction with a VVA under the scope of personal data. Further, since VAs are associated with large size of personal data, the guidelines in addition to adopting the data storage limitation (store only as long as it is necessary to store for purposes for which it is processed) and data minimization principles (limitation of type and quantity of data), have also suggested that data retention periods should be tied to different processing purposes.\nThe Recommendations What is seen last, is remembered best: so here is a shortlist of recommendations on what can be done to protect user privacy and security while using VAs -\n Right to privacy: Even in the absence of a data protection law, users have a fundamental right to privacy which can only be violated if the three-prong test of legality, necessity and proportionality stands fulfilled. This must be kept in mind by data fiduciaries while dealing with personal data. Hence, all companies implementing voice assistants need to closely conduct internal privacy reviews periodically drawing best practices on data protection. Code of Practice: Specifically, when a data protection law is made a specific code of practice for voice assistants in line with best international practices must be specified by the Data Protection Authority under Clause 50 of the Bill. Data retention: Given the unique identifying nature of a voice, existing data retention practices should be amended to adopt a higher threshold for justification of data retention practices with respect to personal data associated with VAs.  Important Documents  The report of the Joint Parliamentary Committee on the Personal Data Protection Bill, 2019 tabled on December 16, 2021 (link) Key Takeaways: The Joint Parliamentary Committee Report and the Data Protection Bill, 2021 #SaveOurPrivacy (link) Comparing the Data Protection Bill, 2021 with its predecessors (link) Our #PrivacyOfThePeople series, where we have discussed the impact of the PDPB on different sections of society (link)        ","wordCount":"2178","inLanguage":"en","datePublished":"2022-01-19T07:16:45Z","dateModified":"2022-01-19T07:16:45Z","author":{"@type":"Person","name":"Anandita Mishra"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://archive.internetfreedom.in/post/privacy-of-the-people-voice-assistants/"},"publisher":{"@type":"Organization","name":"Internet Freedom Foundation","logo":{"@type":"ImageObject","url":"https://archive.internetfreedom.in/iff-icon-125.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://archive.internetfreedom.in accesskey=h title="IFF (Alt + H)"><img src=https://archive.internetfreedom.in/iff-logo-600x250.png alt=logo aria-label=logo height=40>IFF</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://archive.internetfreedom.in/archives title=Archive><span>Archive</span></a></li><li><a href=https://archive.internetfreedom.in/about/ title=About><span>About</span></a></li><li><a href=https://archive.internetfreedom.in/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://archive.internetfreedom.in/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>#PrivacyofthePeople: Alexa, Google, Siri, we hear for you?</h1><div class=post-meta><span title="2022-01-19 07:16:45 +0000 UTC">January 19, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Anandita Mishra&nbsp;|&nbsp;<a href=https://github.com/InternetFreedomFoundation/archive/tree/main/content/post/privacy-of-the-people-voice-assistants.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=tl;dr>tl;dr</a></li><li><a href=#background aria-label=Background>Background</a></li><li><a href=#privacy-concerns-on-consent-and-retention aria-label="Privacy concerns on consent and retention">Privacy concerns on consent and retention</a></li><li><a href=#common-concerns-associated-with-vas aria-label="Common concerns associated with VAs">Common concerns associated with VAs</a></li><li><a href=#the-data-protection-bill-2021-and-vas aria-label="The Data Protection Bill, 2021 and VAs">The Data Protection Bill, 2021 and VAs</a></li><li><a href=#the-recommendations aria-label="The Recommendations">The Recommendations</a></li><li><a href=#important-documents aria-label="Important Documents">Important Documents</a></li></ul></div></details></div><div class=post-content><figure class="kg-gallery-card kg-width-wide"><div class=kg-gallery-container><div class=kg-gallery-row><div class=kg-gallery-image><img src=https://internetfreedom.in/content/images/2022/01/Alexa--Google--Siri--we-hear-for-you--1-.png width=100% height=auto></div></div></div></figure><blockquote><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote></blockquote><h2 id=tldr>tl;dr<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>Voice-enabled AI assistants like Alexa, Google Assistant and Siri reside not only on our smartphones but also in millions of bedrooms. The intimacy they enjoy presents a range of privacy risks that can be mitigated by a user-centric, rights focussed, data protection law. In this <a href=https://internetfreedom.in/tag/privacyofthepeople/>#PrivacyOfThePeople</a> series, we discuss concerns on consent (when and how are such devices collecting data), data retention (storage of parts of audio recordings for undefined periods), cyber security audits (malicious attacks), and data sharing and surveillance (exposure of voice recordings to Voice Assistance training personnel and law enforcement).</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>India is witnessing a <a href=https://www.thinkwithgoogle.com/_qs/documents/7650/Year_in_Search__India_Insights_for_brands_jeEKocQ.pdf>270% year-on-year growth</a> in voice searches. In 2020, 60% of users in India were <a href=https://www.thinkwithgoogle.com/intl/en-apac/country/india/ok-google-how-is-voice-making-technology-more-accessible-in-india/>estimated</a> to be interacting with voice assistants (‘<strong>VA</strong>’) on their smartphones. The Mobile Marketing Association in its ‘<a href=https://www.mmaglobal.com/files/documents/mma_voice_playbook_0.pdf>The Voice Playbook</a>’ (2021) records that smart speakers are being used on a daily basis for listening to music, asking questions, checking the weather, setting a timer, setting an alarm, listening to news and so on. The report points out that the reason for such exponential growth may be convenience (voice is faster than typing) and lack of literacy in India which prompts the rural population to use VAs.</p><p>Mark Weiser who headed Xerox PARC labs explained it well when he stated that, “<em>The most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it.</em>” Given the ubiquity of AI assistants in our daily interactions, we analyse the privacy and security concerns on three predominant, popular deployments, Alexa, Google Assistant, Siri. The basis of this analysis is the Draft Data Protection Bill, 2021 or how an ideal data protection framework can regulate their use, while maintaining innovation and limiting harm.</p><h2 id=privacy-concerns-on-consent-and-retention>Privacy concerns on consent and retention<a hidden class=anchor aria-hidden=true href=#privacy-concerns-on-consent-and-retention>#</a></h2><p>The first privacy concern is regarding data retention. All three VAs store a part of user data including audio recordings for an undefined period even when <a href=https://edpb.europa.eu/system/files/2021-07/edpb_guidelines_202102_on_vva_v2.0_adopted_en.pdf>retention of data indefinitely</a> is against the internationally accepted principle of storage limitation. Their data retention practices are detailed below:</p><figure class="kg-gallery-card kg-width-wide"><div class=kg-gallery-container><div class=kg-gallery-row><div class=kg-gallery-image><img src=https://internetfreedom.in/content/images/2022/01/unnamed.png width=100% height=auto></div></div></div></figure><ol><li><strong>Alexa:</strong> The data collected is associated with Amazon accounts. Amazon stores recordings by default unless users choose to <a href=https://www.theverge.com/2019/5/29/18644027/amazon-alexa-delete-voice-recordings-command-privacy-hub>delete</a> them. They can <a href="https://www.amazon.in/gp/help/customer/display.html?nodeId=201602230">opt-in</a> for auto-deletion of recordings that are older than “3 or 18 months” or choose to not save records at all. Do remember though, that the recordings are still processed on the cloud servers before deletion. Also, the policy is not clear on what happens to the recordings which do not meet this cut-off. Moreover, deletion only applies to voice recordings and Amazon may “retain <em>other</em> records of your Alexa interactions”. Amazon has also previously admitted to <a href=https://www.theverge.com/2019/7/3/20681423/amazon-alexa-echo-chris-coons-data-transcripts-recording-privacy>holding on to data</a> received through Alexa voice interactions despite user-deletion requests. Further, as per the policy, the actions Alexa took in response to your requests can be stored for an undefined period. That means if Alexa repeated any personally identifiable sensitive information for a user, it will be on the cloud server for as long as Amazon wants it.</li><li><strong>Google Assistant:</strong> The voice data collected through Google Assistant is associated with Google accounts. Voice recordings are <a href=https://www.androidpolice.com/2020/08/06/google-opts-all-users-out-of-voice-data-collection-explains-what-it-does-with-the-data/>not stored by default</a> and users have an option to <a href=https://www.theverge.com/2018/7/20/17594802/google-home-how-to-delete-conversations-recorded>delete</a> them. However, if users consent to share their recordings for the purpose of improvement of features, <a href="https://www.youtube.com/watch?v=oqmcvxzbRJs&ab_channel=Google">snippets of such recordings</a> (after being dissociated with users’ accounts) can be saved for an undefined period to improve machine learning. Google’s policy is unclear on retention of other kinds of Google Assistant-related data, but Google’s <a href=https://policies.google.com/privacy#inforetaining>privacy policy</a> does allow for data retention.</li><li><strong>Siri:</strong> A users’ ‘request history’ generated as a result of using Siri is associated with a random-device-generated unique identifier and not with an Apple ID or email address. Request history includes transcripts, audio for users who have opted in to Improve Siri, Siri Data i.e., contact names, names of apps installed on the device, music etc., device specifications, approximate location etc. Voice recordings are saved on Apple’s servers only if one opts in on the ‘<a href=https://www.apple.com/legal/privacy/data/en/improve-siri-dictation/>Improve Siri</a>’ feature. Otherwise, there is an <a href=https://www.apple.com/legal/privacy/data/en/ask-siri-dictation/>option</a> of processing user-voice on the device itself. However, irrespective of this option, the transcripts of interactions are always sent to Apple to process requests. The ‘request history’ is disassociated from the unique identifier after 6 months of its creation, but can be stored for up to 2 years on Apple’s servers to help Apple improve its services. In fact, a small ‘subset’ of these requests may be stored for an <a href=https://www.apple.com/legal/privacy/data/en/ask-siri-dictation/>undefined period</a>. Users do have an option to <em>‘request’</em> deletion of request history (the pop-up states that the data <em>‘will’</em> be deleted), but it is <a href=https://www.theverge.com/2019/8/2/20734681/apple-siri-privacy-settings-how-to-delete-voice-servers>not automatic</a>.</li></ol><p>The second privacy concern is the processing and storage of data without user knowledge and consent. VAs work based on users&rsquo; voices - it is their main feature. All the above-mentioned VAs activate upon hearing a particular activation keyword. Although some of the policies claim that the cloud servers do not store data/voice unless the activation word is detected, there is constant exchange of voice & related data between their cloud servers and the VA device. This is especially concerning in cases of <a href=https://www.theguardian.com/technology/2019/oct/09/alexa-are-you-invading-my-privacy-the-dark-side-of-our-voice-assistants>false activation</a> when data may be getting stored without actual knowledge. The process of activation followed for each VA is detailed below:</p><ol><li><strong>Alexa</strong>: Alexa devices are ordinarily activated by voice on hearing the ‘wake word’ chosen by a user (e.g., Alexa, Amazon, Computer or Echo). Amazon <a href="https://www.amazon.in/gp/help/customer/display.html?nodeId=201602230">claims</a> that till the wake word is detected, no audio is stored or sent to the cloud. The question here is, how is the wake word detected? While that is not answered clearly in the policy, Amazon has described how it verifies a wake word. As per its policy, the Alexa-enabled device starts streaming audio to the cloud upon detection of the wake word. This is where Alexa performs a “cloud verification” to confirm its detection. If the cloud verification does not also detect the wake word, Alexa stops processing the audio and ends the audio stream to the cloud. If the wake word is detected, Alexa attempts to determine when a users’ request ends and stops the audio stream at that point. Interestingly, the policy is silent on what happens to the audio received if the wake word is not verified or incorrectly verified by Alexa.</li><li><strong>Google Assistant</strong>: As per Google’s <a href="https://support.google.com/googlenest/answer/7072285?hl=en&ref_topic=7173611#zippy=%2Cdoes-the-google-assistant-retain-my-audio-recordings%2Ccan-i-look-at-what-ive-asked-my-device-with-the-google-assistant-what-my-google-nest-device-has-heard%2Chow-do-i-delete-my-search-location-history-and-voice-recordings%2Cwhere-are-you-saving-my-data%2Chow-do-i-change-what-google-knows-about-me%2Cwhat-information-does-google-collect-when-i-interact-with-the-google-assistant">policy</a>, the device is on standby mode by default and only activates upon hearing words like “Hey Google” or “Ok Google.” Short snippets of audio (a few seconds) are <em>processed</em> to detect an activation. If no activation is detected, then those audio snippets won’t be sent or saved to Google. If an activation is detected, the Assistant comes out of standby mode to fulfil user requests. The policy is again unclear on how the activation word is detected and where the short snippets are processed. If snippets are processed on Google’s server, the policy must mention how Google deals with the data associated with those snippets.</li><li><strong>Siri</strong>: It can be activated by voice by using the words, “<a href=https://support.apple.com/en-in/HT204389>Hey Siri</a>.” Siri’s <a href=https://www.apple.com/legal/privacy/data/en/ask-siri-dictation/>policy</a> is completely silent on how it processes the activation word. However, Siri has a peculiar feature. Siri Settings of a device clearly states if a user’s voice is processed on their device. If the same is not stated, the voice inputs are sent to and processed on Siri servers. Therefore, if the voice is being processed on the device itself, the issue of processing and storage of data without a user’s knowledge may not be that exacerbated because the device is always with the user. The policy also states that transcripts of voice recordings are always sent to Apple servers. Therefore, in cases of false detection of the activation word, even if the audio is processed on the device, processing and storage of transcripts on servers will still take place without user knowledge.</li></ol><h2 id=common-concerns-associated-with-vas>Common concerns associated with VAs<a hidden class=anchor aria-hidden=true href=#common-concerns-associated-with-vas>#</a></h2><p>The common concerns associated with all VAs independent of their policies are surveillance and security. Human involvement is essential for improvement of Natural Language Processing (allows interaction with a system in a spontaneous way) and machine learning. Employees of Apple, Google and Amazon review randomly selected parts of voice recordings to verify whether VAs processed the voice requests correctly. <a href=https://surfshark.com/blog/voice-assistant-privacy-concerns>Training personnels</a> also listen to the audio, transcribe it, annotate it, and convert it into material that can be used for training voice recognition algorithms. So, even when the voice snippet stored in cloud servers is not associated with an identifier on Siri, or an ID on Google Assist, if the snippet is (say) of your partner saying, “order Raj his favourite Margherita at our Lonavala house”, the training personnel hearing this information will know personally identifiable information about Raj and the speaker. Another risk of exposure is when governments, law enforcement authorities or courts demand audio recordings and associated data for the purpose of <a href=https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/>investigations</a>.</p><p>Security concerns are associated with malware attacks and hacking. Researchers have <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8036736/>found</a> that successful malicious attacks against VAs are becoming more and more sophisticated. The hyperlinked study summarises flaws that allow remote execution of voice commands on Alexa which can enable a malicious user to make online purchases with another user’s VA, discusses security attacks such as ‘phoneme morphing’ in which a VA is tricked into thinking a modified recording of an attacker’s voice is the device’s registered user, and ‘dolphin attack’ in which inaudible voice commands can be used to communicate with VAs, evading detection by human hearing. These attacks allow the extraction of personally identifiable information from unsecured VAs with ease. Therefore, the privacy of voice recordings depends heavily on the security of the servers on which they are stored, and the devices on which they are enabled.</p><h2 id=the-data-protection-bill-2021-and-vas>The Data Protection Bill, 2021 and VAs<a hidden class=anchor aria-hidden=true href=#the-data-protection-bill-2021-and-vas>#</a></h2><p>After two years of its <a href=http://loksabhadocs.nic.in/lobmk/17/Second/SLOB11.12.2019_.pdf>constitution</a> the Joint Parliamentary Committee (“<strong>JPC</strong>”) on the Personal Data Protection Bill, 2019 released its <a href=http://164.100.47.193/lsscommittee/Joint%20Committee%20on%20the%20Personal%20Data%20Protection%20Bill,%202019/17_Joint_Committee_on_the_Personal_Data_Protection_Bill_2019_1.pdf>report</a> in December 2021. The report introduces Data Protection Bill, 2021 (“<strong>Bill</strong>’) incorporating the amendments and omissions suggested by the JPC.</p><p><a href="https://drive.google.com/file/d/16uyML-868dJRP3Nxif1LrfRvoq61dTo1/view?usp=sharing">Clause 17</a> provides users with the right to confirmation and access. This right will allow VA users to demand confirmation from the data fiduciaries on whether their request to delete their data has been processed. Notwithstanding the policies of the data fiduciaries, by virtue of <a href="https://drive.google.com/file/d/16uyML-868dJRP3Nxif1LrfRvoq61dTo1/view?usp=sharing">Clause 18</a>, a VA user can also demand erasure of personal data which is no longer necessary for the purpose for which it was processed.</p><p>As seen above, when VA users consent to the use of audio recordings by Amazon, Apple, and Google for improvement of services, the companies can store small anonymised snippets or subsets of the voice recordings for an undefined period. <a href="https://drive.google.com/file/d/16uyML-868dJRP3Nxif1LrfRvoq61dTo1/view?usp=sharing">Clause 20</a> applies when a user wants to withdraw such consent. Upon withdrawal of consent, it gives the user the right to restrict or prevent processing of their personal data (i.e. one’s voice) by a data fiduciary (provided it does not take away the data fiduciaries rights under this Bill.)</p><p>Pursuant to <a href="https://drive.google.com/file/d/16uyML-868dJRP3Nxif1LrfRvoq61dTo1/view?usp=sharing">Clause 50</a>, the Data Protection Authority must specify a specific code of practice for VAs. The European Data Protection Board has adopted <a href=https://edpb.europa.eu/system/files/2021-07/edpb_guidelines_202102_on_vva_v2.0_adopted_en.pdf>guidelines</a> on virtual voice assistants (‘<strong>VVA</strong>’) which can be used as a guiding document. The guidelines bring all forms of interaction with a VVA under the scope of <em>personal data</em>. Further, since VAs are associated with large size of personal data, the guidelines in addition to adopting the data storage limitation (store only as long as it is necessary to store for purposes for which it is processed) and data minimization principles (limitation of type and quantity of data), have also suggested that data retention periods should be tied to different processing purposes.</p><h2 id=the-recommendations>The Recommendations<a hidden class=anchor aria-hidden=true href=#the-recommendations>#</a></h2><p>What is seen last, is remembered best: so here is a shortlist of recommendations on what can be done to protect user privacy and security while using VAs -</p><ol><li><strong>Right to privacy</strong>: Even in the absence of a data protection law, users have a <a href=https://indiankanoon.org/doc/91938676/>fundamental right to privacy</a> which can only be violated if the three-prong test of legality, necessity and proportionality stands fulfilled. This must be kept in mind by data fiduciaries while dealing with personal data. Hence, all companies implementing voice assistants need to closely conduct internal privacy reviews periodically drawing best practices on data protection.</li><li><strong>Code of Practice</strong>: Specifically, when a data protection law is made a specific code of practice for voice assistants in line with best international practices must be specified by the Data Protection Authority under Clause 50 of the Bill.</li><li><strong>Data retention</strong>: Given the unique identifying nature of a voice, existing data retention practices should be amended to adopt a higher threshold for justification of data retention practices with respect to personal data associated with VAs.</li></ol><h2 id=important-documents>Important Documents<a hidden class=anchor aria-hidden=true href=#important-documents>#</a></h2><ol><li>The report of the Joint Parliamentary Committee on the Personal Data Protection Bill, 2019 tabled on December 16, 2021 (<a href="https://drive.google.com/file/d/1emcAB8HjE2oCC_DI6zR5YPnPQ5iwwwCT/view?usp=sharing">link</a>)</li><li>Key Takeaways: The Joint Parliamentary Committee Report and the Data Protection Bill, 2021 #SaveOurPrivacy (<a href=https://internetfreedom.in/key-takeaways-the-jpc-report-and-the-data-protection-bill-2021-saveourprivacy-2/>link</a>)</li><li>Comparing the Data Protection Bill, 2021 with its predecessors (<a href=https://internetfreedom.in/comparing-pdpb/>link</a>)</li><li>Our #PrivacyOfThePeople series, where we have discussed the impact of the PDPB on different sections of society (<a href=https://internetfreedom.in/tag/privacyofthepeople/>link</a>)</li></ol><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://archive.internetfreedom.in/tags/privacy/>Privacy</a></li><li><a href=https://archive.internetfreedom.in/tags/privacyofthepeople/>PrivacyofThePeople</a></li><li><a href=https://archive.internetfreedom.in/tags/data-protection/>Data Protection</a></li><li><a href=https://archive.internetfreedom.in/tags/data-security/>data security</a></li><li><a href=https://archive.internetfreedom.in/tags/voice-assistant/>voice assistant</a></li><li><a href=https://archive.internetfreedom.in/tags/alexa/>Alexa</a></li><li><a href=https://archive.internetfreedom.in/tags/siri/>Siri</a></li><li><a href=https://archive.internetfreedom.in/tags/google-assistant/>Google Assistant</a></li></ul><nav class=paginav><a class=prev href=https://archive.internetfreedom.in/post/compliance-report-oct/><span class=title>« Prev Page</span><br><span>#SocialMediaComplianceWatch: analysis of Social Media Compliance Reports of October, 2021</span></a>
<a class=next href=https://archive.internetfreedom.in/post/dangers-of-digiyatra/><span class=title>Next Page »</span><br><span>The dangers of DigiYatra & facial recognition enabled paperless air travel #SaveOurPrivacy</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on twitter" href="https://twitter.com/intent/tweet/?text=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f&url=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f&hashtags=Privacy%2cPrivacyofThePeople%2cDataProtection%2cdatasecurity%2cvoiceassistant%2cAlexa%2cSiri%2cGoogleAssistant"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f&title=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f&summary=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f&source=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on reddit" href="https://reddit.com/submit?url=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f&title=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on whatsapp" href="https://api.whatsapp.com/send?text=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f%20-%20https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share #PrivacyofthePeople: Alexa, Google, Siri, we hear for you? on telegram" href="https://telegram.me/share/url?text=%23PrivacyofthePeople%3a%20Alexa%2c%20Google%2c%20Siri%2c%20we%20hear%20for%20you%3f&url=https%3a%2f%2farchive.internetfreedom.in%2fpost%2fprivacy-of-the-people-voice-assistants%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://archive.internetfreedom.in>Internet Freedom Foundation</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>